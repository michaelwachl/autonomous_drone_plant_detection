#!/usr/bin/env python
import sys
import rospy
import time
from geometry_msgs.msg import Twist, PoseStamped, Point, Pose, Quaternion, PoseArray
# import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import PointCloud
from nav_msgs.msg import Path, Odometry
from geometry_msgs.msg import PoseStamped, Point32
from std_msgs.msg import String, Empty, Int8, Header
from visualization_msgs.msg import Marker
from sensor_msgs.msg import Image, ChannelFloat32
from darknet_ros_msgs.msg import BoundingBox, BoundingBoxes, ObjectCount
from sort_track.msg import TrackedBoundingBoxes
from tf.transformations import euler_from_quaternion, quaternion_from_euler
from tf import TransformListener, LookupException, ConnectivityException, ExtrapolationException
import tf.transformations as transformations
from copy import deepcopy
import numpy as np
import cv2


class Scale:
    """Class for marker scale definition (rviz)"""
    x = 0.2
    y = 0.2
    z = 0.2


class TrackContainer:
    """Container for all lines of tracked object"""
    begin_pts = np.empty((0, 3), dtype=np.float32)
    end_pts = np.empty((0, 3), dtype=np.float32)
    frame_pts = np.empty((0, 3), dtype=np.float32)
    class_id = None
    track_id = None


class AllContainer:
    """Container for all objects"""
    track_ids = []
    containers = []


class PlantLocalizer(object):
    """Controller to fly Tello in 3D coordinates"""
    def __init__(self):

        # initialize ROS node
        rospy.init_node('localize_plant_node', anonymous=True)
        # get node name
        self.ns_node = 'localize_plant/'  # rospy.get_namespace()
        # Gets the necessary parameters from .yaml file
        self.latency = rospy.get_param("~latency")
        self.min_line_length = rospy.get_param("~min_line_length")
        self.max_line_length = rospy.get_param("~max_line_length")
        self.line_step = rospy.get_param("~line_step")
        self.discard_margin = rospy.get_param("~discard_margin")     # in pixel
        self.latency_image_position = rospy.get_param('~latency_image_position')
        self.camera_matrix = rospy.get_param('~camera_matrix')
        self.dist_coeffs = rospy.get_param('~distortion_coefficients')
        self.min_number_lines = rospy.get_param('~min_number_lines')
        self.cosine_sim = rospy.get_param('~cosine_sim')
        self.create_pc = rospy.get_param('~create_pc')
        self.debug_out = rospy.get_param('~debug_out')
        self.image_width = rospy.get_param('~image_width')
        self.image_height = rospy.get_param('~image_height')

        # camera matrix for tello single cam
        """              f  x  0 
         camera_matrix = 0  f  y
                         cx cy 1       

        f: focal length
        c: optical center     
        """
        self.camera_matrix = np.array(self.camera_matrix['data'], dtype=np.float32)
        self.camera_matrix = self.camera_matrix.reshape((3, 3))
        self.dist_coeffs = np.array(self.dist_coeffs['data'], dtype=np.float32)
        print(self.camera_matrix)
        """
        # alternative camera parameters
        self.camera_matrix = np.array([[924.873180, 0.000000, 486.997346],
                                       [0.000000, 923.504522, 364.308527],
                                       [0.000000, 0.000000, 1.000000]], dtype=np.float32)
                                       
        self.dist_coeffs = np.array([-0.034749, 0.071514, 0.000363, 0.003131, 0.000000], dtype=np.float32)                             
        """
        # define variables
        self.current_pose = PoseStamped()
        self.current_yaw = 0.0
        self.slam_control = False
        self.first_pointcould = PointCloud()
        header = Header()
        header.stamp = rospy.Time.now()
        header.frame_id = 'map'
        self.first_pointcould.header = header
        self.points = None
        self.first_pointcould.header = header
        self.second_pointcould = PointCloud()
        self.second_pointcould.header.frame_id = 'map'
        self.tracker_pointcould = PointCloud()
        self.tracker_pointcould.header.frame_id = 'map'
        self.line_point = Point32()
        self.ns_tello = 'tello/'
        self.ns_darknet = 'darknet_ros/'
        self.ns_plant_loc = 'plant_localizer/'
        self.bag_time = None
        self.bounding_box_time = rospy.Time()

        # 90 deg around y-axis
        self.pos_listener = TransformListener()
        self.rot_camera = np.array([[0.0, 0.0, -1.0],
                                   [0.0, 1.0, 0.0],
                                   [1.0, 0.0, 0.0]], dtype=np.float32)

        self.slam_pose_topic = 'orb_slam_2_ros/pose'

        self.msg_boundingBoxes = BoundingBoxes()

        # ROS Subs und Pubs initialize
        rospy.Subscriber(self.ns_tello+'odom', Odometry, self.cb_odom, queue_size=1)
        rospy.Subscriber(self.ns_darknet+'found_object', ObjectCount, self.cb_objects, queue_size=1)
        rospy.Subscriber(self.ns_darknet+'bounding_boxes', BoundingBoxes, self.cb_bounding_boxes, queue_size=1)
        rospy.Subscriber('/sort_tracker/tracked_bounding_boxes', TrackedBoundingBoxes, self.cb_tracked_bounding_boxes, queue_size=1)

        self.pub_first_cloud = rospy.Publisher(self.ns_plant_loc + "label_one_cloud", PointCloud, queue_size=10)
        self.pub_second_cloud = rospy.Publisher(self.ns_plant_loc + "label_two_cloud", PointCloud, queue_size=10)
        self.pub_tracker_cloud = rospy.Publisher(self.ns_plant_loc + "tracker_cloud", PointCloud, queue_size=10)
        self.pub_marker = rospy.Publisher('plant_marker', Marker, queue_size=10)

        # containers for localizations
        self.refresh_locations = 0
        self.plant_locations = []
        self.tracked_plant = TrackContainer()
        self.tracked_plants = AllContainer()

        # invert camera matrix for projetion to 3D coordinates
        self.camera_matrix_inv = np.linalg.inv(self.camera_matrix)
        """
        self.test = np.zeros((3, 2), dtype=np.float32)
        self.test[0, :] = [486.99, 364.30]
        self.test[1, :] = [0.0, 0.0]
        self.test[2, :] = [600.0, 400.0]
        self.labels = [0, 1, 0]
        """
        print('init done')
        # keep process alive
        rospy.spin()

    def get_transform_to_map(self, img_time):
        """try to transfrom from current points from camera frame to map frame
        """
        try:
            # latency = rospy.Duration(self.latency)
            # lookup_time = img_time - latency
            self.pos_listener.waitForTransform('map', 'camera_frame', rospy.Time(), rospy.Duration(10))
            # lookup_time = rospy.Time.now() - rospy.Duration(self.latency)
            # trans, rot = self.pos_listener.lookupTransform('map', 'camera_frame', lookup_time)
            # print("look up transform")
            trans, rot = self.pos_listener.lookupTransform('map', 'camera_frame', img_time)
            #trans, rot = self.pos_listener.lookupTransform('map', 'camera_frame', rospy.Time.now())
            # trans, rot = self.pos_listener.lookupTransform('map', 'camera_frame', lookup_time)
            # print (trans, rot)
            # print('current_time: ', rospy.Time.now(), '  time from bounding box:', self.bounding_box_time)
            # trans, rot = self.pos_listener.lo okupTransform('map', 'camera_frame', self.bounding_box_time)
            # Get relative transform between frames
            # offset_to_world = np.matrix(transformations.quaternion_matrix(rot))
            trans_to_map = np.array(transformations.quaternion_matrix(rot))
            # print("Rotation matrix: ", trans_to_map)
            trans_to_map[0, 3] = trans[0]
            trans_to_map[1, 3] = trans[1]
            trans_to_map[2, 3] = trans[2]
            return trans_to_map
        except (LookupException, ConnectivityException, ExtrapolationException) as e:
            print('No Position Data available', e)
        return None

    def undistort_and_project(self, uv_pts):
        """Converts 2D pixel images points to the spherical coordinate system.
        First pixel coordinates are undistorted with calibration matrix and distortion coefficiants.
        Inverse matrix of Intrinsics of camera are taken into account to transform into cartesian vertices and
        then converted to spherical coordinates.
        """
        # (u, v) is the pixel input point
        num_pts = uv_pts.shape[0]
        # uv_pts.shape = (num_pts, 1, 2)

        # do the actual undistortion
        # calculate undistorted pixel coordinates
        uv_pts = cv2.undistortPoints(uv_pts, self.camera_matrix, self.dist_coeffs, P=self.camera_matrix)
        # transform to homogeneous coordinates, add 1
        pts_h = cv2.convertPointsToHomogeneous(np.float32(uv_pts))
        pts_h.shape = (num_pts, 3)
        xyz_pts = np.dot(pts_h, self.camera_matrix_inv.T)
        # print("xyz: ", xyz_pts)
        # transform to homogeneous coordinates, add 1 row
        xyz_pts_h = cv2.convertPointsToHomogeneous(np.float32(xyz_pts))
        xyz_pts_h.shape = (num_pts, 4)
        return xyz_pts_h

    def calculate_and_add_lines(self, points, labels, time_im):
        if not self.create_pc:
            return
        # get current transformation
        camera_to_map = self.get_transform_to_map(time_im)
        # print('Transformation matrix: ', camera_to_map)
        if camera_to_map is None:
            print("Transform not possible")
            return
        # Calculate lines in map coordinates
        # map_points = np.dot(points, camera_to_map.T)
        # print('Map points: ', map_points)
        for i in range(len(labels)):
            # generate and add point cloud
            if labels[i] == 0:
                for scale in np.arange(self.min_line_length, self.max_line_length, self.line_step):
                    # Compose with point to get pose in map frame
                    scaled_point = scale*points[i, :]
                    scaled_point[3] = 1.0
                    point_in_map = np.array(np.dot(scaled_point, camera_to_map.T))
                    self.line_point.x = point_in_map[0]
                    self.line_point.y = point_in_map[1]
                    self.line_point.z = point_in_map[2]
                    if 3.1 > self.line_point.x > 2.9:
                        print('World Point in 3m: ', self.line_point)
                    self.first_pointcould.points.append(deepcopy(self.line_point))
            elif labels[i] == 1:
                for scale in np.arange(self.min_line_length, self.max_line_length, self.line_step):
                    # Compose with point to get pose in map frame
                    scaled_point = scale*points[i, :]
                    scaled_point[3] = 1.0
                    point_in_map = np.array(np.dot(scaled_point, camera_to_map.T))
                    self.line_point.x = point_in_map[0]
                    self.line_point.y = point_in_map[1]
                    self.line_point.z = point_in_map[2]
                    self.second_pointcould.points.append(deepcopy(self.line_point))
        # publish pointclouds
        self.pub_first_cloud.publish(self.first_pointcould)
        self.pub_second_cloud.publish(self.second_pointcould)
        print('New lines published')

    def cosine_similarty(self, a_beg, a_end, b_beg, b_end):
        """returned: cos_sim
                -1 meaning exactly opposite
                1 meaning exactly the same
                0 indicating orthogonality or decorrelation
        """
        # print('cos points a', a_beg, a_end)
        # print('cos points b', b_beg, b_end)
        # Compute cosine similarity
        vec_a = (a_end - a_beg)
        vec_b = (b_end - b_beg)
        n_a = np.linalg.norm(vec_a)  # normalized
        n_b = np.linalg.norm(vec_b)  # normalized
        dot = np.dot(vec_a, vec_b)
        cos_sim = dot / (n_a * n_b)
        return cos_sim

    def add_to_tracker_cloud(self, pt_begin, pt_end):
        x_len = abs(pt_end[0] - pt_begin[0])
        y_len = abs(pt_end[1] - pt_begin[1])
        z_len = abs(pt_end[2] - pt_begin[2])
        line_length = int(np.sqrt((x_len ** 2) + (y_len ** 2) + (z_len ** 2)))  # Length of line
        line_length = 100
        (x, y, z) = (np.linspace(pt_begin[0], pt_end[0], line_length), np.linspace(pt_begin[1], pt_end[1], line_length),
                     np.linspace(pt_begin[2], pt_end[2], line_length))

        for i in range(len(x)):
            # Compose with point to get pose in map frame
            self.line_point.x = x[i]
            self.line_point.y = y[i]
            self.line_point.z = z[i]
            self.tracker_pointcould.points.append(deepcopy(self.line_point))

        self.pub_tracker_cloud.publish(self.tracker_pointcould)

    def add_to_container(self, xyz_h, track_ids, time_im):
        # get current transformation
        camera_to_map = self.get_transform_to_map(time_im)
        # print('Transformation matrix: ', camera_to_map)
        if camera_to_map is None:
            print("Transform not possible")
            return
        line_begin_pts = np.array(np.dot(xyz_h, camera_to_map.T))
        pts_scaled = self.max_line_length*xyz_h
        pts_scaled[:, 3] = 1.0     # contain homogenous dim
        line_end_pts = np.array(np.dot(pts_scaled, camera_to_map.T))
        # iterate and save points
        if len(track_ids) == len(line_begin_pts):
            for i in range(len(line_begin_pts)):
                if track_ids[i] in self.tracked_plants.track_ids:
                    # and (track_ids[i] == 1 or track_ids[i] == 34):
                    # get index
                    index = self.tracked_plants.track_ids.index(track_ids[i])
                    # calculate cosine similarity
                    cosine_sim = self.cosine_similarty(
                        self.tracked_plants.containers[index].begin_pts[-1, :],
                        self.tracked_plants.containers[index].end_pts[-1, :],
                        line_begin_pts[i, :-1],
                        line_end_pts[i, :-1])
                    # print('cosine sim to prev point: ', cosine_sim)
                    # append line if not too similar
                    if cosine_sim < self.cosine_sim:
                        # print('cosine sim to prev point: ', cosine_sim)
                        self.tracked_plants.containers[index].begin_pts = np.append(
                            self.tracked_plants.containers[index].begin_pts, [line_begin_pts[i, :-1]], axis=0)
                        self.tracked_plants.containers[index].end_pts = np.append(
                            self.tracked_plants.containers[index].end_pts, [line_end_pts[i, :-1]], axis=0)
                        self.tracked_plants.containers[index].track_id = track_ids[i]
                        self.add_to_tracker_cloud(line_begin_pts[i, :-1], line_end_pts[i, :-1])
                        print('Point added to id: ', track_ids[i], ' begin: ', line_begin_pts[i, :-1], ' end: ', line_end_pts[i, :-1])
                else:
                    # create plant
                    plant = TrackContainer()
                    print("Shape line point: ", line_begin_pts[i, :-1].shape, line_begin_pts[i, :-1])
                    print("Shape container point: ", plant.begin_pts.shape)
                    plant.begin_pts = np.append(plant.begin_pts, [line_begin_pts[i, :-1]], axis=0)
                    plant.end_pts = np.append(plant.end_pts, [line_end_pts[i, :-1]], axis=0)
                    plant.track_id = track_ids[i]
                    self.tracked_plants.track_ids.append(track_ids[i])
                    self.tracked_plants.containers.append(deepcopy(plant))
                    print('New plant added')

    def update_locations(self):
        print('=========== UPDATE LOCATIONS ===========')
        for plant in self.tracked_plants.containers:
            if len(plant.begin_pts) > self.min_number_lines:
                # intersect
                intersection = self.intersect_lines(plant.begin_pts, plant.end_pts)
                self.plant_locations.append(deepcopy(intersection))
                scale = Scale()
                print('intersection point: ', intersection)
                self.set_maker(plant.track_id, intersection, scale)

    def calculate_center_points(self, bounding_boxes, tracked):
        center_points = np.zeros((len(bounding_boxes), 1, 2), dtype=np.float32)
        frame_points = np.zeros((len(bounding_boxes)*4, 1, 2), dtype=np.float32)
        labels = []
        tracked_ids = []
        i = 0
        for box in bounding_boxes:
            #   --> x
            #   |
            #   v  y
            half_width = int((box.xmax-box.xmin)/2)
            half_height = int((box.ymax-box.ymin)/2)
            center_x = box.xmin + half_width
            center_y = box.ymin + half_height
            center_point = [center_x, center_y]
            center_points[i, :] = center_point
            frame_points[i, :] = [center_x - half_width, center_y]       # left
            frame_points[i+1, :] = [center_x + half_width, center_y]     # right
            frame_points[i+2, :] = [center_x, center_y + half_height]     # top
            frame_points[i+3, :] = [center_x, center_y - half_height]     # bottom
            i += 1
            if tracked:
                labels.append(box.class_id)
                tracked_ids.append(box.track_id)
            else:
                labels.append(box.id)
        return center_points, labels, tracked_ids, frame_points

    def cb_objects(self, msg):
        object_number = msg.count

    def estimate_scale(self, width, height):
        return 1.0

    def filter_bounding_boxes(self, bounding_boxes):
        filtered_boxes = []
        # for box in bounding_boxes:
        for i in range(len(bounding_boxes)):
            #   --> x
            #   |
            #   v  y
            if bounding_boxes[i].xmax < self.image_width - self.discard_margin and \
                    bounding_boxes[i].ymax < self.image_height - self.discard_margin and \
                    bounding_boxes[i].xmin > self.discard_margin and \
                    bounding_boxes[i].ymin > self.discard_margin:
                filtered_boxes.append(bounding_boxes[i])
            # else:
        """
        print("Bounding Box filtered")
        print(filtered_boxes)
        print("Filtered IDS")
        print(filtered_ids)
        """
        return filtered_boxes

    def cb_bounding_boxes(self, msg):
        if len(msg.bounding_boxes) > 0 and self.create_pc:
            # print('bounding box received')
            self.bounding_box_time = msg.header.stamp
            # get time of image of detection for correct time stamp
            image_time = msg.image_header.stamp
            filtered_boxed = self.filter_bounding_boxes(msg.bounding_boxes)
            # if len(filtered_boxed) > 0:
            # center_points, labels, tracked_ids = self.calculate_center_points(filtered_boxed, False)
            center_points, labels, tracked_ids, frame_points = self.calculate_center_points(msg.bounding_boxes, False)
            # print("Center points pixel: ", center_points, "  Labels: ", labels)
            xyz_h = self.undistort_and_project(center_points)
            # add lines to point cloud
            self.calculate_and_add_lines(xyz_h, labels, image_time)

    def cb_tracked_bounding_boxes(self, msg):
        # print('tracked bounding box received:')
        filtered_boxes = self.filter_bounding_boxes(msg.bounding_boxes)
        #print('tracked filtered')
        if len(filtered_boxes) > 0:
            self.bounding_box_time = msg.header.stamp
            # get time of image of detection for correct time stamp
            image_time = msg.image_header.stamp
            center_points, labels, tracked_ids, frame_points = self.calculate_center_points(filtered_boxes, True)
            # print("center points calculated")
            # print("Center points pixel: ", center_points, "  Labels: ", labels)
            xyz_h = self.undistort_and_project(center_points)
            xyy_frame_h = self.undistort_and_project(frame_points)
            self.add_to_container(xyz_h, tracked_ids, image_time)
            # print("added to container")
            self.refresh_locations += 1
            if self.refresh_locations > 100:
                self.update_locations()
                self.refresh_locations = 0

    def cb_odom(self, msg):
        # print('odom callback')
        if self.slam_control == False:
            self.bag_time = msg.header.stamp
            self.current_pose = msg.pose.pose
            # inverse yaw
            self.current_pose.orientation.w = -self.current_pose.orientation.w
            current_orientation = self.current_pose.orientation
            current_orientation_list = [current_orientation.x, current_orientation.y, current_orientation.z,
                                        current_orientation.w]
            current_angles = euler_from_quaternion(current_orientation_list)
            self.current_yaw = current_angles[2]

    def cb_slam_position(self, msg):
        if self.slam_control:
            self.slam_pos = msg.pose.position
            self.slam_quaternion = msg.pose.orientation
            self.slam_orientation_rad_list = euler_from_quaternion(
                [self.slam_quaternion.x, self.slam_quaternion.y, self.slam_quaternion.z, self.slam_quaternion.w])
            self.slam_orientation_rad = Point(self.slam_orientation_rad_list[0], self.slam_orientation_rad_list[1],
                                              self.slam_orientation_rad_list[2])

    def perpendicular_distance(self, intersection, line_points, projectors):
        diff = line_points - intersection[:, 0]  # (a - p)
        temp = np.matmul(projectors, diff[:, :, np.newaxis])  # (I - nnT)(a - p)
        squared_distances = np.matmul(diff[:, np.newaxis], temp)  # (a -p)T(I - nnT)(a - p)
        distances = np.sqrt(squared_distances)
        mean_distance = np.mean(distances)
        std_distance = np.std(distances)
        return distances, mean_distance, std_distance

    def intersect_lines(self, p_beg, p_end):
        """P0 and P1 are NxD arrays defining N lines (start and end point).
        D is the dimension of the space. This function
        returns the least squares intersection of the N
        lines from the system.

        reference: equation 13 in
        https://silo.tips/download/least-squares-intersection-of-lines
        """
        # generate all line direction vectors
        n = (p_end - p_beg) / np.linalg.norm(p_end - p_beg, axis=1)[:, np.newaxis]  # normalized
        #n = (p_end - p_beg) / np.linalg.norm(p_end - p_beg)
        #print("direction vectors: ", n, " shape: ", n.shape)

        # generate the array of all projectors
        projectors = np.eye(n.shape[1]) - n[:, :, np.newaxis] * n[:, np.newaxis]  # I - n*n.T
        # print("projectors: ", projectors, ' shape: ', projectors.shape)

        # generate R matrix and q vector
        R = projectors.sum(axis=0)
        q = (np.matmul(projectors, p_beg[:, :, np.newaxis])).sum(axis=0)

        # solve the least squares problem for the
        # intersection point p: Rp = q
        intersection = np.linalg.lstsq(R, q, rcond=None)[0]

        if self.debug_out:
            distances, mean_distance, std_distance = self.perpendicular_distance(intersection, p_beg, projectors)
            print('Intersection: ', intersection)
            print('Distances: ', distances)
            print('Mean: %s  Std: %s' % (mean_distance, std_distance))
        return intersection

    def set_maker(self, id_marker, position, scale):
        """ Set a marker in rviz with an id
        the position is the center of the object and scale is its dimensions. e.g. scale.z = 1.0 is 1m height
        """
        marker = Marker()
        marker.header.frame_id = "world"
        marker.header.stamp = rospy.Time.now()
        marker.ns = "tomato_plants"
        marker.id = id_marker
        marker.type = 3         # 0=ARROW, 1=CUBE, 2=SPHERE, 3=CYLINDER, 4=LINE_STRIP
        marker.action = 0       # 0 = add/modify, 1 = (deprecated), 2 = delete, 3 = delete all
        marker.pose.position.x = position[0]
        marker.pose.position.y = position[1]
        marker.pose.position.z = position[2]
        marker.pose.orientation.x = 0.0
        marker.pose.orientation.y = 0.0
        marker.pose.orientation.z = 0.0
        marker.pose.orientation.w = 1.0
        marker.scale.x = scale.x
        marker.scale.y = scale.y
        marker.scale.z = scale.z
        marker.color.a = 1.0    # alpha
        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        self.pub_marker.publish(marker)
        print("marker published")


if __name__ == '__main__':
    # Create controller
    # start up the controller node and run until shutdown by interrupt
    try:
        localizer = PlantLocalizer()
        """
        while not rospy.is_shutdown():
            #localizer.undistort_and_project(localizer.test, localizer.labels)
            point = Point32()
            point.x = 1.0
            point.y = 1.0
            scale = Scale()
            localizer.set_maker(1, point, scale)
            rospy.sleep(2)
            print("slept")
        """

    except rospy.ROSInterruptException:
        rospy.loginfo("Plant Localizer node terminated.")
